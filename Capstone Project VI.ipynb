{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["w6K7xa23Elo4"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    -Unsupervised ML - Online Retail Customer Segmentation\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - Unsupervised ML - Online Retail Customer Segmentation\n","##### **Prepared by**     - Anurudra Jena\n","\n"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["In this project, our task is to identify major customer segments on a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n","\n","The data collection was already completed and can be found in many different data/ML workshop websites. e.g.- Kaggle\n","\n","Further tasks to be completed are as follows:\n","1. Preprocessing:\n","Clean and preprocess the data by handling missing values, removing outliers, and normalizing or scaling numerical features.\n","\n","2. Feature Selection:\n","Selecting appropriate features for clustering that can effectively differentiate customers and capture meaningful patterns. For this project, a subset of relevant features such as customer demographics, purchase behavior, or interaction history will be used.\n","\n","3. Generating RFM Score:\n","Just before doing any segmentation/clustering, its recommended that we do an EDA on the data to get a firm understanding of the data and its impact on the business.\n","\n","4. EDA:\n","Creating RFM analysis allows us to compare between potential contributors or clients. It gives organizations a sense of how much revenue comes from repeat customers (versus new customers), and which levers they can pull to try to make customers happier so they become repeat purchasers.\n","\n","5. Choosing a Clustering Algorithm:\n","Selecting a suitable clustering algorithm based on the nature of the data and project requirements. Commonly used algorithms for customer segmentation include K-means clustering, hierarchical clustering, DBSCAN, or Gaussian mixture models (GMM). But for this project we'll be using K-Means clustering.\n","\n","6. Evaluation and Validation:\n","The quality of clustering results using appropriate metrics such as silhouette score, Davies-Bouldin index, or within-cluster sum of squares (WCSS) for K-means will be evaluated.\n","The clusters will be validated by assessing their coherence and interpretability in relation to business objectives and domain knowledge."],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["Provide your GitHub Link here."],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Knowing the data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from datetime import datetime\n","from numpy import math\n","import datetime as dt\n","from sklearn import preprocessing\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import silhouette_score\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","from sklearn import metrics\n","from yellowbrick.cluster import KElbowVisualizer"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["df_raw = pd.read_csv('/content/sample_data/Online Retail.csv', encoding='unicode_escape')"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["df_raw.head()"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["df_raw.shape"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["df_raw.info()"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["df_raw.duplicated().sum()"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["df_raw.isnull().sum()"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.heatmap(df_raw.isna())"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["After looking at the dataset for missing and duplicated values, it can be observed that most of them are pretty negligible with respect to the size of the dataset, except for the CustomerID column which has about 40k missing values.\n","\n","Although it can be said that since the number of missing CustomerID doesn't account for the missing customer sales that didn't happen and we can ofcourse get a greater and much more valid insight if instead of removing those rows, we can add trial customerIDs so that their purchases and inputs can be count for segmentation purpose. Although for the sake of gaining business insights we have to eliminate those customers."],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["df_raw.select_dtypes(include = ['int64','float64']).columns"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_raw.select_dtypes(include = ['object']).columns\n"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"code","source":["df_raw.describe()"],"metadata":{"id":"PJGOS8a1aUAG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["df_raw.nunique()"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# creating a boxplot to check the price distribution density and analyse the ouliers in the group\n","\n","sns.boxplot(x = 'Quantity', data = df_raw)\n","plt.show()\n","\n","# creating a boxplot to check the Minimum_nights distribution density and analyse the ouliers in the group\n","\n","sns.boxplot(x = 'UnitPrice', data = df_raw)\n","plt.show()"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# quantifying the price variable in percentiles to check the outliers in the group\n","\n","df_raw.loc[:,'UnitPrice'].quantile([x for x in np.arange(0.1,1.0,0.1)]+[0.98]+[0.99]+[0.995]+[0.999]+[0.9995]+[0.9999])"],"metadata":{"id":"46ozDTYda5SC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_raw.loc[:,'Quantity'].quantile([x for x in np.arange(0.1,1.0,0.1)]+[0.98]+[0.99]+[0.995]+[0.999]+[0.9995]+[0.9999])"],"metadata":{"id":"uUSiEt-rbLsV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_price_handled = df_raw[(df_raw['UnitPrice'] <= df_raw['UnitPrice'].quantile(0.9995))]"],"metadata":{"id":"j8ZsYg9dbQYg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dropping duplicates first !"],"metadata":{"id":"VYRggWGhcNXS"}},{"cell_type":"code","source":["df_price_handled.drop_duplicates(inplace=True)"],"metadata":{"id":"4Qu7vkWObfPj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["First removing the columns with 350+ null Description values."],"metadata":{"id":"A_r2dyVRw7ha"}},{"cell_type":"code","source":["df_filtered = df_price_handled[df_price_handled['Description'].notna()]"],"metadata":{"id":"hMyQASG0wyrR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we have a DF where only the CustomerID column will be having null values."],"metadata":{"id":"617ZhFB8xCRB"}},{"cell_type":"code","source":["def fill_empty_with_random(df):\n","    for col in df.columns:\n","        if df[col].isna().any():\n","            np.issubdtype(df[col].dtype, np.number)\n","            random_values = np.random.randint(20000, 99999, size=len(df))\n","\n","            df[col].fillna(pd.Series(random_values), inplace=True)\n","\n","    return df\n","\n","df_filtered = fill_empty_with_random(df_filtered)"],"metadata":{"id":"93IQ_CTvvofs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lets check how much null values did our function filled out."],"metadata":{"id":"iyBWWVFxyFfL"}},{"cell_type":"code","source":["df_filtered.isnull().sum()"],"metadata":{"id":"c9AULNQsyCtM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we caan see almost 3363 null values are still there in the CustomerID section maybe because of being different data type. This can be handled by adding more conditionals in the function or using error handling techniques.\n","But for the sake of simplicity, we can simply remove the left out null values which aren't a significant part of the entire dataset."],"metadata":{"id":"Tz0eXbbJyOm6"}},{"cell_type":"markdown","source":["Now all the missing customerID values in the dataset has been filled out with garbage/raandom values which shall act individual customers and would weigh in while we do our segmentation."],"metadata":{"id":"nmWUl1XnfbI0"}},{"cell_type":"code","source":["df_filtered.dropna(inplace = True)"],"metadata":{"id":"bU5lZbq8eUqn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that the null values of CustomerID has been handled, we can remove the null values form the Description column which is a very non-ssignificant part of the entire dataset."],"metadata":{"id":"8F1jhF8WnAll"}},{"cell_type":"code","source":["df_filtered.isnull().sum()"],"metadata":{"id":"aLmy_6zXm5jt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we don't have any more null values, so we can move towards changing required data types.\n"],"metadata":{"id":"h6NbFvu4qEZ6"}},{"cell_type":"code","source":["df_filtered['InvoiceNo'] = df_filtered['InvoiceNo'].astype('str')"],"metadata":{"id":"tCohLNRBol7S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Removing cancelled orders to gain insights only on valid orders."],"metadata":{"id":"7ZIRRpTmqXYM"}},{"cell_type":"code","source":["df_filtered = df_filtered[~df_filtered['InvoiceNo'].str.contains('C')]"],"metadata":{"id":"THXRH5UpqWz_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since more than 40 orders have UnitPrice as zero, those can be removed considering those items are add-ons to some real items."],"metadata":{"id":"WYKobJCWsHAu"}},{"cell_type":"code","source":["df_f = df_filtered[df_filtered['UnitPrice']>0]\n","df_f.head()"],"metadata":{"id":"eEqZ6vwCsE9h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_f.shape\n","df_f.describe()"],"metadata":{"id":"ZdaUnVpjtO-v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["The outliers from the UnitPrice and Quantity column has been rermoved and the missing values from the CustomerID and Description has been handled.\n","\n","In addition to that, the cancelled orders and the orders with zero UnitPrice has also been removed."],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"markdown","source":["Converting Invoice date to Date-time format"],"metadata":{"id":"Y0ixRRJWuaCY"}},{"cell_type":"code","source":["df_f['InvoiceDate'] = pd.to_datetime(df_f['InvoiceDate'], format='mixed')"],"metadata":{"id":"mLNHsrGWugwU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_f['day'] = df_f['InvoiceDate'].dt.day_name()\n","df_f['year'] = df_f['InvoiceDate'].apply(lambda x: x.year)\n","df_f['month_num'] = df_f['InvoiceDate'].apply(lambda x: x.month)\n","df_f['day_num'] = df_f['InvoiceDate'].apply(lambda x: x.day)\n","df_f['hour'] = df_f['InvoiceDate'].apply(lambda x: x.hour)\n","df_f['minute'] = df_f['InvoiceDate'].apply(lambda x: x.minute)\n","df_f['month'] = df_f['InvoiceDate'].dt.month_name()"],"metadata":{"id":"KBDC19dPyyZw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***5.*** ***EDA***"],"metadata":{"id":"5AFb7Imj2o9e"}},{"cell_type":"markdown","source":["**Top 10 Highest selling products**"],"metadata":{"id":"XfnieNOJ2xVU"}},{"cell_type":"code","source":["top10_products = df_f['Description'].value_counts().reset_index().head(10)\n","print(top10_products)"],"metadata":{"id":"BngQvPIt4Cor"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plotting the above data to get a perspective."],"metadata":{"id":"z0aYvS6b5Nv-"}},{"cell_type":"code","source":["plt.figure(figsize = (15,8))\n","sns.barplot(x= top10_products['count'], y = top10_products['Description'])"],"metadata":{"id":"dG1OywJY5MuI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Top 10 spending Customers**"],"metadata":{"id":"HwY8W09uuRdB"}},{"cell_type":"code","source":["top10_customers = df_f['CustomerID'].value_counts().reset_index().head(10)\n","print(top10_customers)"],"metadata":{"id":"zb2pUKPSuQzX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Remember, we had put some random customerID values... ??\n","\n","The range for Random values were from 20000 to 99999, and as we can see none of the top 10 customers are having customer_ID in that range, we can say that our drive to save the data didn't hamper any insights.\n","\n","Now lets plot the above data and get some perspective. We can use Bar chart as the data is quite normalized."],"metadata":{"id":"xXDidNx5zOLG"}},{"cell_type":"code","source":["plt.figure(figsize = (15,8))\n","sns.barplot(x= top10_customers['CustomerID'], y = top10_customers['count'])"],"metadata":{"id":"rjfYMmmsz5lv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Top 5 buying countries**"],"metadata":{"id":"8rm46r0a0KlX"}},{"cell_type":"code","source":["top5_countries = df_f['Country'].value_counts().reset_index().head(5)\n","print(top5_countries)"],"metadata":{"id":"TTZ5bgV80SdC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8, 6))\n","\n","colors = sns.color_palette('pastel')\n","plt.pie(top5_countries['count'], labels=top5_countries['Country'], colors=colors, autopct='%1.1f%%', startangle=140)\n","plt.title('Country retail share')\n","plt.show()"],"metadata":{"id":"cIQ_nhQu0sRF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Monthly retail sales**"],"metadata":{"id":"pfpdFZoN0FZT"}},{"cell_type":"code","source":["monthly_sales = df_f['month'].value_counts().reset_index()\n","print(monthly_sales)"],"metadata":{"id":"9aQlGBst2TLR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plotting the above data:"],"metadata":{"id":"GuiD09jv2vAc"}},{"cell_type":"code","source":["plt.figure(figsize = (15,8))\n","sns.barplot(x= monthly_sales['month'], y = monthly_sales['count'])\n","plt.title(\"Sales count on different months\")"],"metadata":{"id":"yiM_P3EN2rg_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Day-to-Day sales**"],"metadata":{"id":"Of9nLxTl3Ij5"}},{"cell_type":"code","source":["daily_sales = df_f['day'].value_counts().reset_index()\n","print(daily_sales)"],"metadata":{"id":"GAFiksI23HY0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plotting the abpve data for better insights."],"metadata":{"id":"ZT1bInq13yVC"}},{"cell_type":"code","source":["plt.figure(figsize = (15,8))\n","sns.barplot(x= daily_sales['day'], y = daily_sales['count'])\n","plt.title(\"Sales count on different days\")"],"metadata":{"id":"GhaOXd4O34cw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Time of the day sales figures**"],"metadata":{"id":"NXjEPVoS4Ces"}},{"cell_type":"code","source":["df_f['hour'].unique()"],"metadata":{"id":"-G6YRfJX4ZiI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def day_time(time):\n","  if time>=6 and time<=11:\n","    return \"Morning\"\n","  elif time>=12 and time<= 17:\n","    return \"Afternoon\"\n","  else: return \"Evening\"\n","\n","df_f['DayTime'] = df_f['hour'].apply(day_time)"],"metadata":{"id":"j1vYo1Id5Ag3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sales_time = df_f['DayTime'].value_counts().reset_index()\n","print(sales_time)"],"metadata":{"id":"xpjoDKU86Z_Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plotting the above data for better insight:"],"metadata":{"id":"aSkgRp7t6muJ"}},{"cell_type":"code","source":["plt.figure(figsize=(8, 6))\n","\n","colors = sns.color_palette('pastel')\n","plt.pie(sales_time['count'], labels=sales_time['DayTime'], colors=colors, autopct='%1.1f%%', startangle=140)\n","plt.title('Day time Sales')\n","plt.show()"],"metadata":{"id":"aVlj2Jjn6rc9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Average amount spent by each customer**"],"metadata":{"id":"1RrOWnWtXWML"}},{"cell_type":"code","source":["df_f['TotalAmount'] = df_f['Quantity']*df_f['UnitPrice']"],"metadata":{"id":"MZ0GN0HNXyTq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_amt = df_f.groupby('CustomerID')['TotalAmount'].mean().reset_index().rename(columns = {'TotalAmount':'Avg_amt_per_customer'}).sort_values('Avg_amt_per_customer', ascending = False).head(10)\n","avg_amt"],"metadata":{"id":"rYbeFW5LXcBq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plotting the top 10 values of the above data:"],"metadata":{"id":"pt0F44B5Y3Ul"}},{"cell_type":"code","source":["plt.figure(figsize = (15,8))\n","sns.barplot(x= avg_amt['CustomerID'], y = avg_amt['Avg_amt_per_customer'])\n","plt.title(\"Average amount spent by the top 10 customers\")"],"metadata":{"id":"2vhwjpYCY2S4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Observations**"],"metadata":{"id":"ThotgfO57Ury"}},{"cell_type":"markdown","source":["The top 5 selling products all yearr around:\n","1.  WHITE HANGING HEART T-LIGHT HOLDER    282\n","2.            REGENCY CAKESTAND 3 TIER    211\n","3.         HAND WARMER BABUSHKA DESIGN    185\n","4.        SCOTTIE DOG HOT WATER BOTTLE    177\n","5.               HEART OF WICKER SMALL    176\n","\n","The top 5 most spending customers:\n","1.     12748.0    666\n","2.     17841.0    387\n","3.     14606.0    321\n","4.     14911.0    304\n","5.     17850.0    297\n","\n","Top 5 buying Countries:\n","\n","1.  United Kingdom  46045\n","2.         Germany    760\n","3.          France    650\n","4.            EIRE    386\n","5.           Spain    266\n","\n","Most of the sales happen during the fall and within the Afternoon followed by Morning time.\n","\n","The sales rate is increases as the weekend approaches, the highet happens on Fridays and the lowest happens on Sundays, deriving the fact that people happen to gift mostly on weekends and make last minute purchases. Customized offers can be made to rescue the sales figure on Sundays."],"metadata":{"id":"yqJpLDoy7aW3"}},{"cell_type":"markdown","source":["## ***6. RFM Scoring***"],"metadata":{"id":"2j3xHz4RZ5Db"}},{"cell_type":"markdown","source":["RFM analysis numerically ranks a customer in each of these three categories, generally on a scale of 1 to 5 (the higher the number, the better the result). The \"best\" customer would receive a top score in every category.\n","These three RFM factors can be used to reasonably predict how likely (or unlikely) it is that a customer will do business again with a firm or, in the case of a charitable organization, make another donation.\n","\n","The RFM model is based on three quantitative factors:\n","* Recency: How recently a customer has made a purchase\n","* Frequency: How often a customer makes a purchase\n","* Monetary Value: How much money a customer spends on purchases\n","\n","Since UK holds the significant share of sales, we can only focus on the UK Customers"],"metadata":{"id":"ouSHM9umc_GQ"}},{"cell_type":"code","source":["df_f.shape"],"metadata":{"id":"zKoco0-Th4uo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_uk = df_f[df_f['Country'] == 'United Kingdom']\n","\n","# print the shape of the data\n","df_uk.shape"],"metadata":{"id":"cw4eLwiWdI4g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_uk.describe()"],"metadata":{"id":"AJBTg8jwj_di"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To calculate the RFM, we need to select a date from which we can calculate the RFM, and since it is an old dataset, we can select a date with rrespect to the max date of the data set.\n","\n","latest_date = dt.datetime(2011,1,11)\n","\n","df_uk = df_uk.groupby('CustomerID').agg({'InvoiceDate': lambda x: (latest_date - x.max()).days, 'InvoiceNo': lambda x: len(x),\n","                                            'TotalAmount': lambda x: x.sum()})\n","df_uk['InvoiceDate'] = df_uk['InvoiceDate'].astype(int)\n","\n","# rename columns to frequency, recency, monetary\n","df_uk.rename(columns={'InvoiceDate': 'Recency', 'InvoiceNo': 'Frequency', 'TotalAmount': 'Monetary'}, inplace=True)\n"],"metadata":{"id":"U1wWIJEMdm8t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_uk.shape"],"metadata":{"id":"B9Rtcl2sgGqS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plotting the RFM distribution:"],"metadata":{"id":"ls78maOag_Sf"}},{"cell_type":"code","source":["count = 1\n","plt.subplots(figsize=(20,13))\n","for feature in df_uk:\n","  plt.subplot(2,2,count)\n","  sns.distplot(df_uk[feature])\n","  plt.title(f\"Distribution of the variable {feature}\", fontsize=16)\n","  plt.xlabel(f\"{feature}\")\n","  plt.ylabel(\"Density\")\n","  count += 1"],"metadata":{"id":"-MKowcbChE8Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We see that the distribution is uneven, and skewed for Recency and Frequency.\n","To normalize the curves, we can apply Log Transformation to Normalize the data."],"metadata":{"id":"wbZEzyc5ih-A"}},{"cell_type":"code","source":["# since we do not see any negative values in the RFM scorre dataset, but we can ssee zerro value, we can convert all zero values to 1\n","# and directly apply log Transformation to the RFM values\n","\n","def handle_negative(num):\n","  if num <= 0:\n","    return 1\n","  else:\n","    return num\n","\n","# apply the function to recency and monetary columns\n","df_uk['Recency'] = [handle_negative(x) for x in df_uk['Recency']]\n","df_uk['Monetary'] = [handle_negative(x) for x in df_uk['Monetary']]\n","log_df = df_uk[['Recency', 'Frequency', 'Monetary']].apply(np.log, axis=1).round(3)"],"metadata":{"id":"yoCZwB-eihm_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 1\n","plt.subplots(figsize=(20,13))\n","for feature in log_df:\n","  plt.subplot(2,2,count)\n","  sns.distplot(log_df[feature])\n","  plt.title(f\"Distribution of the variable {feature}\", fontsize=16)\n","  plt.xlabel(f\"{feature}\")\n","  plt.ylabel(\"Density\")\n","  count += 1"],"metadata":{"id":"Au9cIED_jctQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Applying log transformation to the original RFM values\n","\n","df_uk['Recency_log'] = df_uk['Recency'].apply(math.log)\n","df_uk['Frequency_log'] = df_uk['Frequency'].apply(math.log)\n","df_uk['Monetary_log'] = df_uk['Monetary'].apply(math.log)"],"metadata":{"id":"ZQsWzw4Ferp1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Scaling the data:\n","X_features = df_uk[['Recency_log', 'Frequency_log', 'Monetary_log']].values\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X_features)"],"metadata":{"id":"sM87gL9dgP4b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **7. Implementing K-Means algorithm for un-supervised learning**"],"metadata":{"id":"FOcC-jzicgSk"}},{"cell_type":"markdown","source":["Before clustering, we have to inspect the RFM Value and apply Elbow/Silhoutte method to determine the number of clusters we want the Customers to be divided into.\n","\n","For this case we'll be applying Elbow method:"],"metadata":{"id":"4uDdFRXrdWfT"}},{"cell_type":"code","source":["# Applying Elbow method\n","\n","SSE = {}\n","for k in range(1,15):\n","  km = KMeans(n_clusters = k, init = 'k-means++', max_iter = 1000)\n","  km = km.fit(X)\n","  SSE[k] = km.inertia_\n","\n","# plot the graph for SSE and number of clusters\n","visualizer = KElbowVisualizer(km, k=(1,15), metric='distortion', timings=False)\n","visualizer.fit(X)\n","visualizer.poof()\n","plt.show()\n"],"metadata":{"id":"JaE010XocgF2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From the Elbow method, we can see that the optimal number of clusters is 4."],"metadata":{"id":"ryiteeKrg0rO"}},{"cell_type":"code","source":["kmeans = KMeans(n_clusters=4)\n","kmeans.fit(X)\n","y_km = kmeans.predict(X)\n","\n","# Plot the clusters\n","plt.figure(figsize=(8, 5))\n","plt.title('Customer Segmentation based on Recency and Frequency')\n","plt.scatter(X[:,0], X[:,1], c=y_km, s=50, cmap='Set1', label='Clusters')\n","\n","# Plot and annotate the centers\n","centers = kmeans.cluster_centers_\n","plt.scatter(centers[:,0], centers[:,1], c='black', s=200, alpha=0.5, marker='x')\n","for i, center in enumerate(centers):\n","    plt.annotate(f'Cluster {i}', (center[0], center[1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n","\n","plt.xlabel('Recency')\n","plt.ylabel('Frequency')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"-XYr5JdjhD4R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we can see after clustering, Cluster 0 & Cluster 3 are coinsiding with each other. Hence we might be lowering the number of clusters to 3."],"metadata":{"id":"rc_oXRa7mM4z"}},{"cell_type":"code","source":["kmeans = KMeans(n_clusters=3)\n","kmeans.fit(X)\n","y_km = kmeans.predict(X)\n","\n","# Plot the clusters\n","plt.figure(figsize=(8, 5))\n","plt.title('Customer Segmentation based on Recency and Frequency')\n","plt.scatter(X[:,0], X[:,1], c=y_km, s=50, cmap='Set1', label='Clusters')\n","\n","# Plot and annotate the centers\n","centers = kmeans.cluster_centers_\n","plt.scatter(centers[:,0], centers[:,1], c='black', s=200, alpha=0.5, marker='x')\n","for i, center in enumerate(centers):\n","    plt.annotate(f'Cluster {i}', (center[0], center[1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n","\n","plt.xlabel('Recency')\n","plt.ylabel('Frequency')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"uvVUGEi_lR58"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Interpreting the Clusters with the help of RFM Group:**"],"metadata":{"id":"3BmVyZKimpqR"}},{"cell_type":"markdown","source":["Splitting the RFM values to 4 quantiles:"],"metadata":{"id":"RhRruppbm1yn"}},{"cell_type":"markdown","source":[],"metadata":{"id":"JC-Xy5dYm7HF"}},{"cell_type":"code","source":["quantiles = df_uk.quantile(q=[0.25,0.5,0.75])\n","quantiles = quantiles.to_dict()\n","\n","def RScore(x,p,d):\n","  if x <= d[p][0.25]:\n","    return 1\n","  elif x <= d[p][0.5]:\n","    return 2\n","  elif x <= d[p][0.75]:\n","    return 3\n","  else:\n","    return 4\n","def FnMScore(x,p,d):\n","  if x <= d[p][0.25]:\n","    return 4\n","  elif x <= d[p][0.5]:\n","    return 3\n","  elif x <= d[p][0.75]:\n","    return 2\n","  else:\n","    return 1"],"metadata":{"id":"pgh210-Cmj7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_uk['R'] = df_uk['Recency'].apply(RScore, args=('Recency', quantiles, ))\n","df_uk['F'] = df_uk['Frequency'].apply(FnMScore, args=('Frequency', quantiles, ))\n","df_uk['M'] = df_uk['Monetary'].apply(FnMScore, args=('Monetary', quantiles, ))\n","df_uk.reset_index().head()"],"metadata":{"id":"UrWnttKGoZ4G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Calculating RFM Group and RFM Score from the RFM segmentation:**"],"metadata":{"id":"kn1-GQXW1XRy"}},{"cell_type":"code","source":["# add RFM group column\n","df_uk['RFMGroup'] = df_uk['R'].map(str) + df_uk['F'].map(str) + df_uk['M'].map(str)\n","\n","# calculate RFM score from RFM group column\n","df_uk['RFMScore'] = df_uk[['R', 'F', 'M']].sum(axis=1)\n","df_uk.reset_index().head()"],"metadata":{"id":"7AMgsWE611DY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#finding the clusters from the dataset\n","\n","df_uk['Cluster'] = kmeans.labels_\n","df_uk.head()"],"metadata":{"id":"t3pktygI2GP4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.countplot(x = df_uk['Cluster'])"],"metadata":{"id":"hdSBwD7S2JXR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Grouping by cluster label and calculate mean\n","cluster_averages = df_uk.groupby('Cluster').mean()\n","cluster_averages"],"metadata":{"id":"gFuXNOzx2qki"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **8. Interpretetion**"],"metadata":{"id":"z4KaR4qh3Yjs"}},{"cell_type":"markdown","source":["Cluster 0:\n","\n","* Recency: High (average around 30 days)\n","* Frequency: Low (average around 1 transactions)\n","* Monetary: Low (average around $16)\n","* Interpretation: Customers in this group are likely to be 'At-Risk' or 'Lapssesd' customers. They haven't made any recent purchase and when they did they neither purchased anything frequently nor they spent much. Engaging them with reactivation campaigns or exploring why they haven't returned can be a strategic move.\n","But since the data is from an online gifting retail site, it can be derived that people dont buy gifts much frequently, unless they have any occasions to suffice the reaoning. Hence, if the former move to engage the customers doesn't work, seasonal offers and campaigns can be initiated for this segment of customers to have their loyalty to buy from this store whenever they have to buy gifts.\n","\n","\n","Cluster 1:\n","\n","* Recency: High (average around 25 days)\n","* Frequency: High (average around 33 transactions)\n","* Monetary: High (average around $700)\n","* Interpretetion:  This cluster represents your 'Champions' or 'Loyal' customers. They shop frequently, recently, and spend the most. They are the most valuable segment, likely to respond positively to new offers, up-sell and cross-sell opportunities. This segment might consist of other third party retailers, so maintaining their high engagement level is crucial, and they can also be targeted for feedback or as brand ambassadors.\n","\n","Cluster 2:\n","\n","* Recency: Low (average around 4 days)\n","* Frequency: Moderate (average around 2 transactions)\n","* Monetary: Moderate (average around $13)\n","* Interpretetion: Customers in this cluster can be seen as 'Potential Loyalists' or 'Promising' customers. They have a balanced score in all three RFM metrics. These customers have the potential to become more valuable if properly engaged. Tailored marketing strategies, loyalty programs, and incentives to increase their purchase frequency and value can be effective.\n","\n"],"metadata":{"id":"n5fHAihX3ous"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["* The above above clustering is done with RFM data using K-Means Clustering as all 3 will provide the best information for the business requirement.\n","\n","* Cluster 0 has high Recency, low frequency and monetary value thus depicting the most risked class of the entire sample consisting of above 13k customers.\n","* Cluster 1 has high values in all recency, frequency and monetary value depicting about 1000+ golden customers.\n","* Cluster 2 has low recency and moderaate frequency and monetary values consisting of about 2500+ loyal customers."],"metadata":{"id":"Fjb1IsQkh3yE"}}]}